from typing import Iterable, Optional, List, Dict, Any
from chromadb.errors import InvalidDimensionException

from ai.config import AISettings
from ai.services.vectorstore import get_chroma_client, get_collection
from ai.services.embeddings import embed_texts

try:
    from openai import OpenAI
    from openai import RateLimitError, APIError
except Exception:
    OpenAI = None  # type: ignore
    RateLimitError = None  # type: ignore
    APIError = None  # type: ignore


class RAGPipeline:
    """
    Minimal RAG pipeline scaffold.
    Backend A can extend methods to add embeddings, retrievers, and LLM calls.
    """

    def __init__(self, settings: AISettings):
        self.settings = settings
        self.client = get_chroma_client(settings)
        self.collection = get_collection(self.client, settings)

    def _recreate_collection_on_dimension_mismatch(self, e: InvalidDimensionException) -> None:
        """Recreates the collection if a dimension mismatch occurs."""
        print(f"Warning: {e}. Attempting to recreate collection '{self.collection.name}'...")
        self.client.delete_collection(name=self.collection.name)
        self.collection = get_collection(self.client, self.settings)
        print(f"Collection '{self.collection.name}' recreated. Please re-ingest data.")

    def ingest_texts(
        self,
        texts: Iterable[str],
        *,
        course_id: str,
        metadata: Optional[dict] = None,
    ) -> dict:
        """
        Ingest texts with embeddings and course-scoped metadata.
        
        IDs are generated to be unique:
        - If metadata has segment_index: use "{course_id}-seg-{segment_index}"
        - If metadata has page_number: use "{course_id}-page-{page_number}"
        - Otherwise: use "{course_id}-doc-{i}" with auto-increment
        """
        entries = list(texts)
        if not entries:
            return {"ingested": 0}

        md = metadata or {}
        md.setdefault("course_id", course_id)

        embeddings = embed_texts(entries, self.settings)

        # Generate unique IDs based on metadata
        ids = []
        metadatas = []
        
        for i, entry in enumerate(entries):
            current_metadata = {**md, "course_id": course_id} # Ensure course_id is always present
            
            # Use segment_index or page_number if available for unique ID
            if current_metadata.get("segment_index") is not None:
                doc_id = f"{course_id}-seg-{current_metadata['segment_index']}"
            elif current_metadata.get("page_number") is not None:
                doc_id = f"{course_id}-page-{current_metadata['page_number']}"
            elif current_metadata.get("type") == "persona":
                doc_id = f"{course_id}-persona"
            else:
                # Fallback: use index (may cause overwrites if called multiple times without unique metadata)
                doc_id = f"{course_id}-doc-{i}"
            
            ids.append(doc_id)
            metadatas.append(current_metadata)

        try:
            self.collection.upsert(
                ids=ids,
                documents=entries,
                metadatas=metadatas,
                embeddings=embeddings,
            )
        except InvalidDimensionException as e:
            self._recreate_collection_on_dimension_mismatch(e)
            return {"ingested": 0, "error": "Collection recreated due to dimension mismatch. Please re-ingest."}
        except (RateLimitError, APIError) as e:
            error_msg = f"OpenAI API ì˜¤ë¥˜ (ì„ë² ë”©): {str(e)}"
            print(f"ERROR [Ingest]: {error_msg}")
            return {"ingested": 0, "error": error_msg}

        return {"ingested": len(entries)}

    def query(
        self, 
        question: str, 
        *, 
        course_id: str, 
        k: int = 4,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        current_time: Optional[float] = None
    ) -> dict:
        """
        Retrieval with course_id filter + LLM synthesis.
        Supports conversation history for context-aware responses.
        
        Args:
            question: Current question
            course_id: Course identifier
            k: Number of documents to retrieve
            conversation_history: List of previous messages in format [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
        """
        try:
            # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ingest_textsì™€ ë™ì¼í•œ ë°©ì‹)
            try:
                query_embeddings = embed_texts([question], self.settings)
            except ValueError as e:
                # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë“± ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì‹œ
                error_msg = str(e)
                if "í• ë‹¹ëŸ‰" in error_msg or "quota" in error_msg.lower() or "insufficient_quota" in error_msg.lower():
                    detailed_msg = (
                        "âš ï¸ OpenAI API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                        "í•´ê²° ë°©ë²•:\n"
                        "1. OpenAI ëŒ€ì‹œë³´ë“œ(https://platform.openai.com/account/billing)ì—ì„œ í¬ë ˆë”§ ì”ì•¡ í™•ì¸\n"
                        "2. ê²°ì œ ì •ë³´ ë“±ë¡ ë° í¬ë ˆë”§ ì¶”ê°€\n"
                        "3. Rate Limits(https://platform.openai.com/account/limits) í™•ì¸\n\n"
                        f"ì—ëŸ¬ ìƒì„¸: {error_msg}"
                    )
                else:
                    detailed_msg = f"âš ï¸ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
                
                return {
                    "question": question,
                    "documents": [],
                    "metadatas": [],
                    "answer": detailed_msg,
                }
            results = self.collection.query(
                query_embeddings=query_embeddings,
                n_results=k + 1,  # í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ í¬í•¨ì„ ìœ„í•´ +1
                include=["documents", "metadatas", "distances"],
                where={"course_id": course_id},
            )
        except ValueError as e:
            # API í‚¤ë‚˜ í• ë‹¹ëŸ‰ ê´€ë ¨ ì—ëŸ¬
            error_msg = str(e)
            return {
                "question": question,
                "documents": [],
                "metadatas": [],
                "answer": f"âš ï¸ {error_msg}",
            }
        except Exception as exc:
            # If collection dimension mismatch occurs (old collection), recreate and return placeholder
            if isinstance(exc, InvalidDimensionException):
                # Recreate collection with current embedding model name suffix
                self.collection = get_collection(self.client, self.settings)
                return {
                    "question": question,
                    "documents": [],
                    "metadatas": [],
                    "answer": "ë²¡í„° ì»¬ë ‰ì…˜ì„ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
                }
            raise
        docs_all = results.get("documents", []) or [[]]
        metas_all = results.get("metadatas", []) or [[]]
        distances_all = results.get("distances", []) or [[]]
        docs: List[str] = docs_all[0] if docs_all else []
        metas: List[Dict[str, Any]] = metas_all[0] if metas_all else []
        distances: List[float] = distances_all[0] if distances_all else []
        
        # ë””ë²„ê¹…: ê²€ìƒ‰ ê²°ê³¼ ë¡œê·¸
        print(f"[RAG DEBUG] Query: '{question[:50]}...' (course_id={course_id})")
        print(f"[RAG DEBUG] Found {len(docs)} documents")
        if docs:
            for i, (doc, meta, dist) in enumerate(zip(docs[:3], metas[:3], distances[:3])):
                source = meta.get("source", "unknown")
                start_time = meta.get("start_time")
                print(f"[RAG DEBUG] Doc {i+1}: {doc[:100]}... (source={source}, time={start_time}s, distance={dist:.4f})")
        else:
            print(f"[RAG DEBUG] âš ï¸ No documents found for course_id={course_id}")
            # ë²¡í„° DBì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            try:
                all_docs = self.collection.get(
                    where={"course_id": course_id},
                    limit=1
                )
                if not all_docs.get("ids") or len(all_docs["ids"]) == 0:
                    print(f"[RAG DEBUG] âŒ No documents in vector DB for course_id={course_id}. Course may not be processed yet.")
                else:
                    print(f"[RAG DEBUG] âœ… Vector DB has documents for course_id={course_id}, but search returned nothing. This may indicate an embedding mismatch.")
            except Exception as e:
                print(f"[RAG DEBUG] âš ï¸ Could not check vector DB: {e}")
        
        # í˜ë¥´ì†Œë‚˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³„ë„ ê²€ìƒ‰ (ì§ˆë¬¸ê³¼ ê´€ê³„ì—†ì´ í•­ìƒ ê°€ì ¸ì˜¤ê¸°)
        # âš ï¸ query_textsë¥¼ ì‚¬ìš©í•˜ë©´ ChromaDBê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
        # get() ë©”ì„œë“œë§Œ ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ API í˜¸ì¶œ ë°©ì§€
        persona_doc = None
        try:
            # IDë¡œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (ì„ë² ë”© ìƒì„± ì—†ìŒ)
            persona_results = self.collection.get(
                ids=[f"{course_id}-persona"],
                include=["documents", "metadatas"],
            )
            if persona_results.get("documents") and len(persona_results["documents"]) > 0:
                persona_doc = persona_results["documents"][0]
                print(f"[RAG DEBUG] âœ… í˜ë¥´ì†Œë‚˜ë¥¼ IDë¡œ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤ (course_id={course_id}, ì„ë² ë”© í˜¸ì¶œ ì—†ìŒ)")
            else:
                print(f"[RAG DEBUG] âš ï¸ í˜ë¥´ì†Œë‚˜ê°€ ë²¡í„° DBì— ì—†ìŠµë‹ˆë‹¤ (course_id={course_id})")
        except Exception as e:
            # get()ì´ ì‹¤íŒ¨í•˜ë©´ (ì˜ˆ: IDê°€ ì—†ê±°ë‚˜ ì»¬ë ‰ì…˜ ë¬¸ì œ) í˜ë¥´ì†Œë‚˜ ì—†ì´ ì§„í–‰
            print(f"[RAG DEBUG] âš ï¸ í˜ë¥´ì†Œë‚˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (get ì‹¤íŒ¨): {e}")
            # âš ï¸ query_textsë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - ë¶ˆí•„ìš”í•œ ì„ë² ë”© API í˜¸ì¶œ ë°©ì§€
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í˜ë¥´ì†Œë‚˜ ì œê±° ë° ì‹œê°„ ê¸°ë°˜ í•„í„°ë§/ì •ë ¬
        filtered_docs = []
        filtered_metas = []
        doc_scores = []  # ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        
        for i, doc in enumerate(docs):
            meta = metas[i] if i < len(metas) else {}
            if meta.get("type") != "persona":
                # ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (current_timeì´ ìˆëŠ” ê²½ìš°)
                score = 0.0
                if current_time is not None and current_time > 0:
                    start_time = meta.get("start_time")
                    end_time = meta.get("end_time")
                    if start_time is not None or end_time is not None:
                        # í˜„ì¬ ì‹œê°„ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°
                        if start_time is not None and end_time is not None:
                            # segment ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
                            if start_time <= current_time <= end_time:
                                score = 100.0
                            else:
                                # ê±°ë¦¬ì— ë”°ë¼ ì ìˆ˜ ê°ì†Œ
                                mid_time = (start_time + end_time) / 2
                                distance = abs(mid_time - current_time)
                                score = max(0, 100.0 - distance / 10)  # 10ì´ˆë‹¹ 10ì  ê°ì†Œ
                        elif start_time is not None:
                            distance = abs(start_time - current_time)
                            score = max(0, 100.0 - distance / 10)
                        elif end_time is not None:
                            distance = abs(end_time - current_time)
                            score = max(0, 100.0 - distance / 10)
                
                filtered_docs.append(doc)
                filtered_metas.append(meta)
                doc_scores.append(score)
        
        # ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ê°€ ìˆìœ¼ë©´ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
        if current_time is not None and current_time > 0 and any(s > 0 for s in doc_scores):
            # ì ìˆ˜ì™€ ê±°ë¦¬ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì •ë ¬
            sorted_items = sorted(
                zip(filtered_docs, filtered_metas, doc_scores),
                key=lambda x: (x[2], -x[1].get("start_time", 0) if x[1].get("start_time") else 0),
                reverse=True
            )
            filtered_docs = [doc for doc, _, _ in sorted_items]
            filtered_metas = [meta for _, meta, _ in sorted_items]
            print(f"[RAG DEBUG] ğŸ“ Time-based sorting applied (current_time={current_time}s), top score: {max(doc_scores) if doc_scores else 0:.1f}")
        
        answer = self._llm_answer(
            question, 
            filtered_docs, 
            filtered_metas, 
            course_id,
            conversation_history=conversation_history,
            persona_doc=persona_doc  # ëª…ì‹œì ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ì „ë‹¬
        )
        return {
            "question": question,
            "documents": docs,
            "metadatas": metas,
            "answer": answer,
        }

    def _llm_answer(
        self, 
        question: str, 
        docs: List[str], 
        metas: List[Dict[str, Any]], 
        course_id: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        persona_doc: Optional[str] = None
    ) -> str:
        """
        LLM synthesis with persona prompt and conversation history.
        Audio knowledge takes priority, GPT knowledge is supplementary.
        """
        if OpenAI is None or not self.settings.openai_api_key:
            return "LLM placeholder: OPENAI_API_KEYê°€ ì—†ì–´ì„œ ê¸°ë³¸ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤."

        context_parts = []
        for i, doc in enumerate(docs):
            meta = metas[i] if i < len(metas) else {}
            src = meta.get("source") or meta.get("filename") or ""
            ts = meta.get("start_time")
            ctx = f"[{src} @ {ts}s] {doc}" if ts is not None else doc
            context_parts.append(ctx)
        context = "\n".join(context_parts) if context_parts else ""

        # ì €ì¥ëœ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ìˆìœ¼ë©´), ì—†ìœ¼ë©´ ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ ìƒì„±
        if persona_doc:
            persona = persona_doc
            print(f"[RAG DEBUG] âœ… ì €ì¥ëœ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (course_id={course_id})")
        else:
            # í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ ìƒì„± (fallback)
            print(f"[RAG DEBUG] âš ï¸ ì €ì¥ëœ í˜ë¥´ì†Œë‚˜ë¥¼ ì°¾ì§€ ëª»í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ ìƒì„± (fallback, course_id={course_id})")
            persona = self.generate_persona_prompt(course_id=course_id, sample_texts=docs)
        
        # ì˜¤ë””ì˜¤ ì§€ì‹ ìš°ì„ , GPT ì§€ì‹ ë³´ì¡° í”„ë¡¬í”„íŠ¸
        if context:
            knowledge_instruction = (
                "ì¤‘ìš”: ì•„ë˜ 'ê°•ì˜ ì»¨í…ìŠ¤íŠ¸'ì— ìˆëŠ” ë‚´ìš©ì´ ê°€ì¥ ìš°ì„ ìˆœìœ„ê°€ ë†’ìŠµë‹ˆë‹¤. "
                "ë¨¼ì € ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ìœ¼ì„¸ìš”. "
                "ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ì— ëª…í™•í•œ ë‹µì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. "
                "ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì´ í•„ìš”í•  ë•Œë§Œ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”.\n\n"
                "ê°•ì˜ ì»¨í…ìŠ¤íŠ¸:\n"
                f"{context}\n\n"
                "ìœ„ ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. "
                "ê°•ì˜ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ê±°ë‚˜ ìš”ì•½í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
            )
        else:
            knowledge_instruction = (
                "âš ï¸ ê²½ê³ : ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                "ì´ëŠ” ê°•ì˜ê°€ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ë²¡í„° DBì— ë°ì´í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                "ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ê°•ì˜ ë²”ìœ„ì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ì„ ëª…ì‹œí•˜ì„¸ìš”. "
                "ê°•ì˜ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
            )
            print(f"[RAG DEBUG] âš ï¸ No context found for course_id={course_id}, question: {question[:50]}")
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ (ìƒìœ„ ë ˆë²¨ì—ì„œ transcript íŒŒì¼ ì‚¬ìš©í•˜ë„ë¡)
            answer = knowledge_instruction
            return {
                "question": question,
                "documents": [],
                "metadatas": [],
                "answer": answer,
            }
        
        sys_prompt = (
            f"{persona}\n\n"
            "ìœ„ ë§íˆ¬ ì§€ì‹œì‚¬í•­ì„ ì •í™•íˆ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.\n\n"
            f"{knowledge_instruction}\n\n"
            "ë‹µë³€ ê·œì¹™:\n"
            "- ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
            "- ê°•ì˜ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë³´ì™„ ê°€ëŠ¥í•˜ì§€ë§Œ, ê°•ì˜ ë‚´ìš©ì„ì„ ê°•ì¡°í•˜ì„¸ìš”.\n"
            "- ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.\n"
            "- ì½”ìŠ¤ ë²”ìœ„ ë°– ì§ˆë¬¸ì€ ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "- ì´ì „ ëŒ€í™” ë‚´ìš©ë„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”."
        )

        # ë©”ì‹œì§€ êµ¬ì„± (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
        messages = [{"role": "system", "content": sys_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10ê°œë§Œ ìœ ì§€)
        if conversation_history:
            # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨ (í† í° ì œí•œ ê³ ë ¤)
            recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
            for msg in recent_history:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role in ["user", "assistant"] and content:
                    messages.append({"role": role, "content": content})
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": question})

        client = OpenAI(api_key=self.settings.openai_api_key)
        try:
            resp = client.chat.completions.create(
                model=self.settings.llm_model,
                messages=messages,
                temperature=0.3,
            )
            return resp.choices[0].message.content
        except RateLimitError as e:
            error_msg = f"OpenAI API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤: {str(e)}"
            print(f"ERROR [LLM]: {error_msg}")
            return f"âš ï¸ {error_msg}"
        except APIError as e:
            error_msg = f"OpenAI API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            print(f"ERROR [LLM]: {error_msg}")
            return f"âš ï¸ {error_msg}"
        except Exception as e:
            error_msg = str(e)
            if "insufficient_quota" in error_msg or "quota" in error_msg.lower():
                return "âš ï¸ OpenAI API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. OpenAI ê³„ì •ì˜ í¬ë ˆë”§ì„ í™•ì¸í•˜ê±°ë‚˜ ê²°ì œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”."
            elif "rate_limit" in error_msg.lower() or "429" in error_msg:
                return "âš ï¸ OpenAI API Rate Limit ì´ˆê³¼: ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
            else:
                return f"âš ï¸ LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"

    def generate_persona_prompt(
        self, *, course_id: str, sample_texts: list[str]
    ) -> str:
        """
        Analyze speaking style from sample texts and generate persona prompt.
        Uses LLM to extract stylistic patterns (speech patterns, tone, expressions).
        """
        if not sample_texts:
            return f"ë‹¹ì‹ ì€ course_id={course_id} ê°•ì‚¬ì˜ ë§íˆ¬ë¥¼ ëª¨ë°©í•œ AIì…ë‹ˆë‹¤."
        
        if OpenAI is None or not self.settings.openai_api_key:
            # Fallback to simple prompt if API key is missing
            sample = sample_texts[0][:500] if sample_texts else ""
            return (
                f"ë‹¹ì‹ ì€ course_id={course_id} ê°•ì‚¬ì˜ ë§íˆ¬ë¥¼ ëª¨ë°©í•œ AIì…ë‹ˆë‹¤. "
                f"ì•„ë˜ ìƒ˜í”Œì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:\n{sample}"
            )
        
        # Combine sample texts (up to 3000 chars to avoid token limits)
        combined_text = "\n\n".join(sample_texts)
        if len(combined_text) > 3000:
            combined_text = combined_text[:3000] + "..."
        
        # Use LLM to analyze speaking style
        client = OpenAI(api_key=self.settings.openai_api_key)
        
        analysis_prompt = f"""ë‹¤ìŒì€ ê°•ì‚¬ì˜ ê°•ì˜ í…ìŠ¤íŠ¸ ìƒ˜í”Œì…ë‹ˆë‹¤. ì´ ê°•ì‚¬ì˜ ë§íˆ¬ì™€ ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„í•  ìš”ì†Œ:
1. ì¢…ê²°ì–´ë¯¸ íŒ¨í„´ (ì˜ˆ: "-ìŠµë‹ˆë‹¤", "-ì–´ìš”", "-ì£ ", "-ë„¤ìš”" ë“±)
2. ì–´íˆ¬ (ì •ì¤‘í•¨, ì¹œê·¼í•¨, ê²©ì‹, ìºì£¼ì–¼ ë“±)
3. ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ì´ë‚˜ ìŠµê´€ì  ë§íˆ¬
4. ë¬¸ì¥ êµ¬ì¡° (ì§§ì€ ë¬¸ì¥ vs ê¸´ ë¬¸ì¥)
5. íŠ¹ì§•ì ì¸ ë§ë²„ë¦‡ì´ë‚˜ ë°˜ë³µë˜ëŠ” í‘œí˜„

ê°•ì˜ ìƒ˜í”Œ:
{combined_text}

ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
- ì¢…ê²°ì–´ë¯¸: [ë¶„ì„ ê²°ê³¼]
- ì–´íˆ¬: [ë¶„ì„ ê²°ê³¼]
- ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„: [ë¶„ì„ ê²°ê³¼]
- ë¬¸ì¥ êµ¬ì¡°: [ë¶„ì„ ê²°ê³¼]
- íŠ¹ì§•: [ë¶„ì„ ê²°ê³¼]

ì´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì´ ê°•ì‚¬ì˜ ë§íˆ¬ë¥¼ ëª¨ë°©í•˜ëŠ” ë°©ë²•ì„ ìš”ì•½í•´ì£¼ì„¸ìš”."""
        
        try:
            resp = client.chat.completions.create(
                model=self.settings.llm_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì–¸ì–´í•™ìì´ì ìŠ¤íƒ€ì¼ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ë§íˆ¬ì™€ ìŠ¤íƒ€ì¼ì„ ì •í™•í•˜ê²Œ ë¶„ì„í•©ë‹ˆë‹¤.",
                    },
                    {"role": "user", "content": analysis_prompt},
                ],
                temperature=0.3,
            )
            style_analysis = resp.choices[0].message.content
            
            # Generate persona prompt based on analysis
            persona_instruction = f"""ë‹¹ì‹ ì€ course_id={course_id} ê°•ì‚¬ì˜ ë§íˆ¬ì™€ ìŠ¤íƒ€ì¼ì„ ì •í™•í•˜ê²Œ ëª¨ë°©í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.

ê°•ì‚¬ ë§íˆ¬ ë¶„ì„:
{style_analysis}

ìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê·œì¹™ì„ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”:
1. ë¶„ì„ëœ ì¢…ê²°ì–´ë¯¸ íŒ¨í„´ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
2. ë¶„ì„ëœ ì–´íˆ¬ë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”
3. ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„ì´ë‚˜ íŠ¹ì§•ì ì¸ ë§ë²„ë¦‡ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ì„¸ìš”
4. ë¬¸ì¥ êµ¬ì¡°ë„ ì›ë³¸ê³¼ ìœ ì‚¬í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
5. ê°•ì‚¬ì˜ ê°œì„±ê³¼ íŠ¹ì§•ì„ ë°˜ì˜í•˜ì—¬ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”"""
            
            return persona_instruction
        except (RateLimitError, APIError) as e:
            error_msg = f"OpenAI API ì˜¤ë¥˜ (í˜ë¥´ì†Œë‚˜ ìƒì„±): {str(e)}"
            print(f"ERROR [Persona]: {error_msg}")
            sample = sample_texts[0][:500] if sample_texts else ""
            return (
                f"ë‹¹ì‹ ì€ course_id={course_id} ê°•ì‚¬ì˜ ë§íˆ¬ë¥¼ ëª¨ë°©í•œ AIì…ë‹ˆë‹¤. "
                f"í˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}. "
                f"ì•„ë˜ ìƒ˜í”Œì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:\n{sample}"
            )
        except Exception as e:
            print(f"Warning: Failed to analyze persona style: {e}")
            # Fallback to simple prompt
            sample = sample_texts[0][:500] if sample_texts else ""
            return (
                f"ë‹¹ì‹ ì€ course_id={course_id} ê°•ì‚¬ì˜ ë§íˆ¬ë¥¼ ëª¨ë°©í•œ AIì…ë‹ˆë‹¤. "
                f"ì•„ë˜ ìƒ˜í”Œì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:\n{sample}"
            )

